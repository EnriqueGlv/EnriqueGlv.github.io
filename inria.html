
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Efficient Scheduling of Transformers using StarPU</title>
  <link rel="icon" type="image/x-icon" href="assets/favicon.ico">
  <link rel="stylesheet" href="project.css">
  <link rel="stylesheet" href="style.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
  <link rel="stylesheet" href="https://unpkg.com/prismjs@1.24.1/themes/prism.css">
  <link rel="stylesheet" href="admon.css">
  <!-- Google's Material Icons -->
  <link href="https://cdn.jsdelivr.net/npm/@mdi/font@7.4.47/css/materialdesignicons.min.css" rel="stylesheet">


</head>
<body>

  <!-- Navigation Bar -->
  <nav class="navbar">
    <div class="container">
      <a class="navbar-brand" href="index.html">Enrique Galvez</a>
      <ul class="navbar-links">
        <li><a href="index.html#about">About</a></li>
        <li><a href="index.html#projects">Projects</a></li>
      </ul>
    </div>
  </nav>
  
  <!-- Project Content -->
  <section class="project-content-section">
    <div class="project-container">
      <div id="project-content" class="markdown-body">
        <h1>Efficient scheduling of Transformers with StarPU</h1>
<p>The last year at ENS de Lyon consists in a specific training to computer science research, composed of 2 long internships (6 monthes). For the first half of this year, I chose to work as a research intern at <a href="https://www.inria.fr/fr/centre-inria-universite-bordeaux">Inria Bordeaux</a> in the TOPAL team (HPC).</p>
<p>TOPAL team has an expert knowledge in high performance scheduling for mathematical operations, with a group of researchers interested in the scheduling of AI algorithms. My aim during this internship was to study the particular case of the scheduling of decoder-only transformers (e.g. GPT-2) with a high performance scheduling tool called <a href="https://starpu.gitlabpages.inria.fr/">StarPU</a>, developped at Inria Bordeaux.</p>
<h2>The structure of decoder-only transformers</h2>
<p>Transformers as introduced in the paper <a href="https://arxiv.org/abs/1706.03762">Attention is All you Need</a> are composed of 2 distinct parts: an <strong>encoder</strong> that processes the input sequence to generate contextualized<br>
representations, and a <strong>decoder</strong> that generates the output sequence based on these representations. However, in language generation tasks, generated token depends only on the previous tokens, eliminating the need for an encoder to process the entire input sequence.</p>
<!-- This observation is the reason why OpenAI's GPT and Meta's Llama models are decoder-only transformers. -->
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>OpenAI's GPT and Meta's Llama models are based on a decoder-only transformer structure.</p>
</div>
<p>A decoder-only transformer is composed of several parts described in Figure 1.</p>
<p><img src="../assets/inria/transformer.png" alt="transf"></p>
<p>The first element of this block is the <strong>tokenizer</strong>, which transforms the text sequence into small pieces of information called tokens. Then, an <strong>embedding</strong> is computed to transform the text into numerical vectors. The embedding usually has good properties: if two words have a close meaning, the distance between their corresponding vector representation should be small. Modern networks also include at this step a <strong>positionnal encoding</strong> to keep track of the position of the token inside the sequence, considering that the meaning of a piece of text depends on its position in the text.</p>
<p>The next part of the transformer structure is the <strong>multi-head self-attention</strong> layer. It splits the input into multiple “attention heads”, each performing scaled dot-product attention independently, enabling the model to capture diverse relationships and patterns in the data. Each attention head interprets its input vector as a query, key, value (Q, K, V) tuple in order to extract context- specific information by weighting V by an attention score computed using Q and K.</p>
<p>This attention block is followed by a <strong>feed-forward network</strong>. The combination of the attention block and the feed-forward network is a Transformer Block, which is repeated N times to constitute the whole network. At the end of the network, linear and softmax layers are used to extract the output, which is usually a probability vector.</p>
<h2>The scheduling problem for Deep Neural Networks</h2>
<p>Because of the complexity of deep neural networks execution, finding an optimal scheduling for these tasks is a very active research question. A particular case of this pronlem targets distributed environments, which allow higher-scale computing capabilities but require a special attention at optimizing communication costs.</p>
<p><img src="../assets/inria/datapar.png" alt="datapar"></p>
<p>A first strategy used to train DNNs on a distributed environment is <strong>Data Parallelism</strong>, illustrated in Figure 2. This method consists in splitting the input in several batces, and run an entire step for a given batch on each device in parallel. Weights are then gathered to compute the model update that should stay the same in each device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Distributed Data Parallelism (DDP) is the default strategy used by pytorch to run distributed training accross several devices like GPUs.</p>
</div>
<p><img src="../assets/inria/pipelinepar.png" alt="pipeline par"></p>
<p>Another parallelism strategy is <strong>Pipeline  Parallelism</strong>, described in Figure 3. Here, each device is in charge of executing one layer of the network for a given input batch. In this mode, the execution of layer <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.01968em;">l</span></span></span></span> for a given batch cannot be executed before the execution of layer <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">l-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mbin">−</span><span class="mord mathrm">1</span></span></span></span> on this batch. Training also adds as dependencies the backward pass for previous batch, resulting in complex dependencies between pipeline tasks.</p>
<p><img src="../assets/inria/tensorpar.png" alt="tensor par"></p>
<p>A third popular strategy is <strong>Tensor Parallelism</strong>, described in Figure 4. The idea of this strategy is to split the layers in several devices and compute for each layer of the network part of the result in each device. This approach is the more complex to implement because it involves complex dependencies for each split operation.</p>
<div class="admonition summary">
<p class="admonition-title">Task-based parallelism</p>
<p>My work focused on another strategy, called <strong>task-based parallelism</strong>, described in the next session.</p>
</div>
<!-- ![seq par](../assets/inria/seqpar.png) -->
<h2>The tools StarPU and NNTile for scheduling DNNs</h2>
<p>Previous strategies consider the scheduling problem as a global problem. Indeed, in previous approaches, scheduling is thought at the scale of the network. Another approach, referred to as <strong>task-based parallelism</strong>, consists in submitting atomic tasks for operations on small pieces of input and then use an external engine to schedule the tasks.</p>
<p>This approach can be achieved by using a tool such as <a href="https://starpu.gitlabpages.inria.fr/">StarPU</a>, which provides an API to declare atomic operations, and allow their execution thanks to high-performance scheduling algorithms written by experts.</p>
<p><img src="../assets/inria/starpu.png" alt="starpu"></p>
<p>In order to study the execution of deep neural networks with a task-based parallelism strategy, I used the project <a href="https://github.com/nntile/nntile">NNTile</a> as a starting point for my work. NNTile is a deep learning library where each atomic operation (Add, GEMM, ReLU, copy, transpose...) is considered as a StarPU task which is submitted to StarPU's scheduler when the network is executed.</p>
<h2>Achieving high performance scheduling for DNNs</h2>
<p>Achieving high performance scheduling with a task-based parallelism strategy is quite a challenge because the scheduling is done without any global knowledge about the tasks to execute. However, some factors like <strong>task granularity</strong>, <strong>kernel fusion</strong> or <strong>communication overhead</strong> can be adjusted and have turned out to be critical in terms of performance.</p>
<p><img src="../assets/inria/fuselinear.png" alt="fuse linear"></p>
<p>As an example, kernel fusion can be used to optimize the execution of a DNN. In above picture, three tasks are submitted: GEMM, Add and ReLU. In a regular setup, the scheduler will choose how to schedule these tasks on pieces of the input. When the kernels are fused, these tasks are grouped in a single task for each piece of input, executed at once in a single computing unit. The benefits of kernel fusion are multiple: it improves data locality, reduces the cost of StarPU overhead and reduces the cost of inter-devices communications.</p>
<p>By following a similar process at several levels of the network, I managed to observe some speedups against NNTile's implementations. The details of my work, including experiments and pseudo-code are available in my <a href="contents/inria-report.pdf">internship report</a>.</p>
<h2>Additionnal resources</h2>
<p>If you want to learn more about my work, here are some useful resources:</p>
<ul>
<li><a href="contents/inria-report.pdf">Internship Report</a></li>
<li><a href="https://github.com/nntile/nntile">NNTile repository</a></li>
<li><a href="https://starpu.gitlabpages.inria.fr/">StarPU website</a></li>
</ul>
<h2>Special thanks</h2>
<p>I am grateful to my internship supervisors Olivier Beaumont and Julia Gusak for this highly valuable opportunity, as well as the whole TOPAL team for their dedication in the follow-up of my internship.</p>
<!-- ![alt](assets/inria-img.png) -->
<!-- ## Details
- **Framework**: NNTile
- **Focus**: Efficient scheduling of the elementary operations of the attention layer of transformers.

## Report



```python
def abc():
    for i in range(5):
        return i
```

### test

lkgngd:

- dggd


## test

### subtest

blabla

### Convergence of the Series

The convergence of the series $\sum \frac{1}{n^2}$ can be expressed as:

$$\sum_{n=1}^\infty \frac{1}{n^2} = \frac{\pi^2}{6}$$

This result is known as the Basel problem, solved by Leonhard Euler.

Here is inline math: $E = mc^2$.

And here is block math:

$$
\int_0^\infty e^{-x^2} \, dx = \frac{\sqrt{\pi}}{2}
$$

# Admonition Examples

## Tip

!!! tip 'OUAUA'
    *here be dragons* -->
      </div>
    </div>
  </section>

  <div class="quick-nav">
    <a href="index.html#projects" class="quick-nav-button">← Back to Projects</a>
  </div>

  <!-- Footer -->
  <footer class="footer">
    <div class="bot-container">
      <p>&copy; 2023 Enrique Galvez. All rights reserved.</p>
      <div class="social-icons">
        <a href="https://www.linkedin.com/in/enrique-galvez-b25817248" target="_blank">LinkedIn</a>
        <a href="https://github.com/EnriqueGlv" target="_blank">GitHub</a>
      </div>
    </div>
  </footer>

  <!-- Scripts -->
  <script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script>
  <script>
    window.addEventListener("DOMContentLoaded", () => {
      new ClipboardJS(".markdown-it-code-copy");
    });
    </script>
</body>
</html>
    